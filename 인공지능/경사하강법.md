## 경사 하강법

경사하강법은 손실 함수또는 비용 함수라 불리는 목적 함수를 정의하고, 이 함수의 값이 최소화되는 파라미터를 찾는 방법이다.

> 손실 (loss) : 실제값과 모델로 예측한 값이 얼마나 차이가 나는가를 나타내는 값으로, 손실이 작을수록 예측값이 정확한 것이다.<br>비용(cost, error)은 손실을 전체 데이터에 대해 구한 경우이며 비용을 함수로 나타낸 것을 손실 함수또는 비용 함수라고 한다.

함수의 최소값을 찾기 위해 임의의 위치에서 시작해서 기울기를 따라 조금씩 더 낮은 위치로 내려가며 극값에 이를 때까지 반복시킨다.

손실 함수는 인공지능의 파라미터를 통하여 나온 예측 값과 실제 값의 차이이기 때문에, 특정 파라미터를 통하여 나온 손실 함수 값이 가장 낮은 곳이 바로 **최적의 파라미터**라고 할 수 있다.

**경사 하강법 동작 순서**

1. 가중치 초기화
    - 0이나 랜덤 값으로 초기화해준다.
2. 비용함수 계산
    - 만약 현재 위치의 기울기가 **음수**라면 파라미터를 **증가**시키면 최솟값을 찾을 수 있다.
    - 반대로 기울기가 **양수**라면 파라미터를 **감소**시키면 최솟값을 찾을 수 있다.
    - **따라서 해당 파라미터에서 학습률 * 기울기를 빼면 최솟값이 되는 장소를 찾을 수 있다.**
        
        <img width="257" alt="image" src="https://user-images.githubusercontent.com/81006587/230717745-2b10da10-eba5-4a0b-bd0e-b20a45140983.png">


        
3. 가중치 갱신
    - 기울기는 음수 값을 가진다. (오른쪽으로 이동(하강)하게 하기 위해 기울기에 - 를 붙여준다.)
    - 가중치를 조금씩 움직이는 것을 반복하다보면 최저점에 접근할 수 있다.
4. 2~3 과정을 지정한 횟수나 비용함수값이 일정 임계값 이하로 수렴할때까지 반복한다.
    - 전체를 한번 도는걸 1 epoch이라 한다.

**경사 하강법의 문제점**

- 모든 데이터를 적용하여 변화량을 구하기 떄문에 연산량이 많이 필요하다.
- 초기 W값에 따라 지역적 최소값에 빠지는 경우가 발생한다.
    - 이를 해결하기 위해 학습률을 상태에 따라 적응적으로 조절할 필요가 있다. (Stochastic Gradient Descent)
    
        <img width="595" alt="image" src="https://user-images.githubusercontent.com/81006587/230717724-4ef4924a-c178-44d1-aabb-b091b4b4799c.png">
    

## 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)

- 전체 학습 데이터를 사용하지 않고 확률적으로 선택한 샘플의 일부만을 사용하고, 진동하며 결과값으로 수렴한다.
- 일부 데이터만 사용하기 때문에 학습 속도가 매우 빠르다.
- 일반적인 경사 하강법과 반대로 local minimum에 빠지더라도 쉽게 빠져나올 수 있어서 global minimum을 찾을 가능성이 더 크다.
- 손실 함수가 최솟값에 가는 과정이 불안정하다 보니 최적해(global minimum)에 정확히 도달하지 못할 가능성이 있다.
- 결과의 진폭이 크고 불안하다는 단점이 있다. (오차율이 크다)

확률적 경사 하강법의 노이즈를 줄이면서도 전체 배치보다 더 효율적인 방법으로는 미니배치 경사 하강법이 있다.

## 미니배치 경사 하강법

![image](https://user-images.githubusercontent.com/81006587/230538678-c5917ce6-926a-48bc-991b-6bfbae2012a0.png)

SGD와 BGD의 절충안으로 배치 크기를 줄여 확률적 경사 하강법을 이용하는 방법이다.

전체 데이터를 작은 그룹으로 나누고, 작은 그룹 단위로 가중치를 갱신한다. 

전체 데이터를 batch_size개씩 나눠 배치로 학습 시키는 방법이고, 배치 크기는 사용자가 지정한다. 일반적으로는 메모리가 감당할 수 있는 정도로 결정한다.

**미니배치 경사 하강법 특징**
- 전체 데이터셋을 대상으로 한 SGD 보다 parameter 공간에서 shooting이 줄어든다.(미니배치의 손실 값 평균에 대해 경사 하강을 진행하기 때문에)
- BGD에 비해 Local Minima를 어느정도 회피할 수 있다.
- 최적해에 더 가까이 도달할 수 있으나 local optima 현상이 발생할 수 있다. local optima의 문제는 무수히 많은 임의의 parameter로부터 시작하면 해결된다. (학습량 늘리기)
