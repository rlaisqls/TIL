임베딩은 자연어의 통계적 패턴을 숫자 벡터로 바꾼 결과이다.

### 고려하는 정보

임베딩을 만들 때 쓰는 통계 정보는 크게 세 가지가 있다. 첫째는 문장에 어떤 단어가 많이 쓰였는지이고, 둘째는 단어가 어떤 순서로 등장하는지이며, 마지막은 문장에 어떤 단어가 같이 나타났는지와 관련한 정보다.

1. 백오브워즈(bag of words) 가정

    - 수학에서 bag이란 중복 원소를 허용한 집합을 뜻한다.
    - 백오브워즈 가정에서는 어떤 단어가 많이 쓰였는지 정보를 중시하고, 단어의 등장 순서는 무시한다.
    - 단어 빈도 또는 등장 여부를 그대로 임베딩을 쓰는 것은 문서의 주제를 반영하지 못 할 수도 있다는 단점이 있다. (e.g. 은, 는과 같은 조사는 문서의 주제와 상관 없이 자주 등장)
    - 이러한 단점을 보환하기 위해 TF-IDF(Term Frequency-Inverse Document Frequency)라는 기법을 사용한다. TF란 어떤 단어가 특정 문서에서 쓰인 빈도를 나타내고, DF는 단어가 나타난 문서의 수를 뜻한다. 그리고 IDF는 N/DF에 로그를 취하여 그 값이 클수록 특이한 단어라는 뜻이다. 이 값을 사용하여 정보성이 있는 단어의 빈도에 더 가중치를 줄 수 있다.
    - 백오브워즈 가정의 뉴럴 네트워크 버전인 Deep Averaging Network라는 것도 있다.

2. 언어 모델

    - 언어 모델은 단어의 등장 순서를 학습해 주어진 단어 시퀀스가 얼마나 자연스러운지 확률을 부여한다. ELMo, GPT 등과 같은 뉴럴 네트워크 기반의 언어 모델이 여기에 해당한다.
    - 단어가 n개 주어졌다면 언어 모델은 n개 단어가 동시에 나타날 확률을 반환한다.
    - n-gram이란 n개 단어는 뜻하는 용어다. 경우에 따라 n-gram은 n-gram에 기반한 언어 모델을 의미하기도 한다. 긴 문장에 대한 각 단어의 연결 가능성을 고려한 필요가 없다면, 직전 n-1개 단어의 확률로 계산하는 n-gram 모델로 문제를 해결할 수 있다. (이는 한 상태의 확률이 그 직전 상태에만 의존한다는 마코프 가정에 기반한다.)
    - 하지만 데이터에 한 번도 등장하지 않는 n-gram이 존재하면 예측 단계에서 의도치 않은 결과가 나올 수 있다. 이를 위해 백옾, 스무딩 등의 방식이 제안됐다.  

3. 분포 가정

    - 문장에서 어떤 단어가 같이 쓰였는지를 중요하게 따진다. 단어의 의미를 주면 문맥을 통해 유추해볼 수 있다고 보는 것이다.
    - 언어학자들은 형태소를 분석할 때 게열관계(해당 형태소 자리에 다른 형태소가 대치될 수 있는가)와 해당 단어가 문장 내에서 점하는 역할 기준으로 사용하는데, 이는 말뭉치의 분포 정보로 추측할 수 있다. 즉, 단어의 분포를 임베딩 정보의 담는 것은 언어학에서 의미를 가지는 최소 단위인 형태소의 기능을 충분히 나타낼 수 있다
    - 점별 상호 정보량(Pointwise Mutual Information)은 두 확률변수 사이의 상관성을 상관성을 계량화하는 단위이다. 두 확률변수과 완전히 독립인 경우 그 값이 0이다.
    - 분포 가정의 대표적인 모델은 2013년 구글 연구 팀이 발표한 Word2Vec이라는 임베딩 기법이다. CBOW 모델은 문맥 단어로 타깃 단어는 예측하는 과정으로 학습하고, Skip-gram 모델은 타깃 단어로 문맥 단어가 무엇일지 예측하는 과정에서 학습된다. 둘 모두 특정단어 주변의 문맥, 즉 분포 정보를 임베딩에 함축한다.

### 단어 수준 임베딩

모델 종류

- **NPLM**: 뉴럴 네트워크 기반의 임베딩 기법, 단어 시퀀스가 주어졌을 때 다음 단어가 무엇인지 맞추는 과정에서 학습된다.
- **Werd2Vec Skip-gram**: 타깃 단어와 문맥 단어 쌍이 주어졌을 때 해당 쌍이 포지티브 샘플인지, 네거티브 샘플인지 이진 분류하는 과정에서 학습된다. NPLM보다 학습 파라미터 종류와 크기가 훨씬 작고 효율적인 학습이 가능하다.
- **FastText**: 각 단어를 문자 단위의 n-gram으로 표현하는 임베딩 기법이다. 조사나 어미가 발달한 한국어에 좋은 성능을 낸다.
- **GloVe**: Word2Vec과 잠재 의미 분석 두 기법의 단점을 극복하여, '임베딩된 단어 벡터간 유사도 측정을 수원하게 하면서도 말뭉치 전체의 통계 정보를 잘 반영하게' 하기 위해 임베딩 된 두 단어 벡터의 내적이 말뭉치 전체에서의 동시 등장 빈도의 로그 값이 되도록 목적함수를 정의했다고 한다.
- **Swivel**: 점별 상호 정보량(PMI) 행렬을 분해하는 과정에서 학습된다. 말뭉치에 동시 등장한 경우가 한 건도 없는 단어쌍의 PMI는 음의 무한대로 발산하는데, 이 모델은 그 경우를 없애기 위해 목적함수를 조정하였다 한다.

> **잠재 의미 분석**: 단어-문서, TF-IDF, 단어-문맥 행렬 같은 커다란 행렬에 차원 축소 방법의 일종인 특이값 분해를 수행해 데이터의 차원 수를 줄여 계산 효율성을 키우고 행간의 잠재 의미를 이끌어내는 방법론이다. 단어-문서 행렬이나 단어-문맥 행렬에 특이값 분해를 시행한 뒤 그 결과 벡터는 단어 임베딩으로 사용할 수 있다.

평가 방법

- **단어 유사도 평가**: 일련의 단어 쌍을 미리 구성한 후 사람이 평가한 점수와 비교
- **단어 유추 평가**: 임베딩은 벡터이기 때문에 사칙연산을 할 수 있는데, (왕-남자+여자 = 여왕) 이를 통해 단어들 사이의 의미적, 문법적 관계를 기준으로 평가하는 방식이다.

### 문장 수준 임베딩

> 문서 임베딩을 만들기 위한 잠재 의미 분석은 단어-문서 행렬이나 TF-IDF 행렬에 특이값 분해를 시행하고, 축소된 이 행렬에서 문서에 대응하는 벡터를 취하는 방식이다.

- **Doc2Vec**: 이전 k개 단어들과 문서 ID를 넣어서 다음 단어를 예측하거나(PV-DM), 문서 ID를 가지고 주변 단어들을 맞추는(PV-DBOW) 과정에서 학습한다. 결과적으로 문서 ID에 해당하는 문서 임베딩엔 무서에 등장하는 모든 단어들의 의미 정보가 반영된다.
- **잠재 디리클레 할당(LDA)**: 각 문서에 어떤 주제들이 존재하는지에 대한 확률 모형, 문서를 주제 확률 분포로 나타내 각각을 벡터화한다는 점에서 LDA를 임베딩 기법의 일종으로 이해할 수 있다.
- **ELMo**: 입력 단어 시퀀스 다음에 어떤 단어가 올지 낮추는 과정에서 학습된다. ElMo의 입력은 문자 단위, 출력은 토큰 단위이고, 문자단위 CNN, LSTM, Elo 레이어 세 요소로 구성되어 있다.
- **트렌스포머 네트워크**: Scaled Dot-Product Attention과 멀티에드 어텐션, Positoin-wise Feedforward Networks 세 요소로 구서오딘 블록이다. 입력 문자의 의미적, 문법적 관계 추출에 뛰어난 성능을 보여 주목받았다. GPT, BERT 등이 트랜스포머 네트워크를 기본 블록으로 사용한다.
- **BERT**: 입력 문장을 양방향으로 분석하는 모델이다. 주어진 시퀀스 다음 단어를 맞추는 것에서 벗어나, 일단 문장 전체를 모델에 알려주고, 빈칸에 해당하는 단어가 어떤 단어일지 예측하는 과정에서 학습한다.

---
참고

- [한국어 임베딩](https://ratsgo.github.io/embedding)
- <https://nlp-study.tistory.com/19>
- <https://vickiboykis.com/what_are_embeddings/>
- <https://simonwillison.net/2023/Oct/23/embeddings/>
